---
title: "ML_basics"
author: "Giulia Solinas"
date: "2023-11-09"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown


Here the code about the dataset mnist

```{r}
library(dslabs)

mnist <- dslabs::read_mnist()
ncol(mnist$train$images)
```

## BINARY OUTCOME: Assessing the model with accuracy with accuracy, sensitivity, specificity and balanced accuracy


Overall accuracy can sometimes be a deceptive measure because of unbalanced classes.
A general improvement to using overall accuracy is to study sensitivity and specificity separately.

A confusion matrix tabulates each combination of prediction and actual value. You can create a confusion matrix in R using the table() function or the confusionMatrix() function from the caret package. The confusionMatrix() function also provides additional information such as sensitivity and specificity.

```{r}
# tabulate each combination of prediction and actual value
table(predicted = y_hat, actual = test_set$sex)
test_set %>% 
  mutate(y_hat = y_hat) %>%
  group_by(sex) %>% 
  summarize(accuracy = mean(y_hat == sex))
prev <- mean(y == "Male")

confusionMatrix(data = y_hat, reference = test_set$sex)

```       
If your training data is biased in some way, you are likely to develop algorithms that are biased as well.

```{r}
# save the confusion matrix as an object
cm <- confusionMatrix(data = y_hat, reference = test_set$sex)

# access specific metrics
cm$overall["Accuracy"]

cm$byClass[c("Sensitivity","Specificity", "Prevalence")]

```

### Sensitivity and specificity: definitions

Sensitivity is typically quantified by the true positives divided by the sum of true positives plus false negative: the proportion of actual positives that are called positives. This quantity is referred to as the true positive rate, TPR. Or it's sometimes called *recall*.

Specificity is defined as the true negatives divided by the true negatives plus the false positives or the proportion of negatives that are called negatives. This quantity is also called the true negative rate.

However, there's another way of quantifying specificity, which is the true positives divided by the true positives plus false positive or the proportion of outcomes called positives that are actually positive. This quantity is referred to as the positive predictive value, or PPV. And it's also called *precision*.

### Prevalence
Another important summary that can be extracted from the confusion matrix is the prevalence, defined as the proportion of positives. 

In our sex and height example, when we use overall accuracy, the fact that our prevalence, the proportion of females, was too low turned out to be a problem.

Note that unlike the true positive rate and the true negative rate, precision depends on prevalence, since higher prevalence implies you can get higher precision even when guessing.

### Balanced accuracy

Balanced accuracy is defined as the average of the true positive rate and the true negative rate. It's also called the balanced error rate. It's a useful summary because it's not affected by prevalence.

See this [page](https://rafalab.dfci.harvard.edu/dsbook/introduction-to-machine-learning.html#balanced-accuracy-and-f_1-score) for the theoretical explanation. .

```{r}
# maximize F-score
cutoff <- seq(61, 70)
F_1 <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train_set$height > x, "Male", "Female") %>% 
    factor(levels = levels(test_set$sex))
  F_meas(data = y_hat, reference = factor(train_set$sex))
})

data.frame(cutoff, F_1) %>% 
  ggplot(aes(cutoff, F_1)) + 
  geom_point() + 
  geom_line()

max(F_1)

best_cutoff_2 <- cutoff[which.max(F_1)]
best_cutoff_2

y_hat <- ifelse(test_set$height > best_cutoff_2, "Male", "Female") %>% 
  factor(levels = levels(test_set$sex))
sensitivity(data = y_hat, reference = test_set$sex)
specificity(data = y_hat, reference = test_set$sex)

```
Because specificity and sensitivity are rates, it's more appropriate to compute the harmonic average. In fact, the *F1 score*, a widely used one number summary, is the harmonic average of precision and recall.

The F1 score can be adapted to weigh specificity and sensitivity differently. To do this, we define a constant-- we'll call it beta--to represent how much more important sensitivity is compared to specificity, and then we re-write the weighted harmonic average.

### Prevalence matters in practice

A machine learning algorithm with very high sensitivity and specificity may not be useful in practice when prevalence is close to either 0 or 1. For example, if you develop an algorithm for disease diagnosis with very high sensitivity, but the prevalence of the disease is pretty low, then the precision of your algorithm is probably very low based on Bayes' theorem.

### ROC curve

A very common approach to evaluating accuracy and F1-score is to compare them graphically by plotting both. A widely used plot that does this is the receiver operating characteristic (ROC) curve. The ROC curve plots sensitivity (TPR) versus 1 - specificity, also known as the false positive rate (FPR).

However, ROC curves have one weakness and it is that neither of the measures plotted depend on prevalence. In cases in which prevalence matters, we may instead make a precision-recall plot, which has a similar idea with ROC curve.

Install two packages to build the graph

```{r}

library(pROC)
library(ggplot2)
library(plotROC)
```


Here we build the curve

```{r}
p <- 0.9
n <- length(test_index)
y_hat <- sample(c("Male", "Female"), n, replace = TRUE, prob=c(p, 1-p)) %>% 
  factor(levels = levels(test_set$sex))
mean(y_hat == test_set$sex)

# ROC curve
probs <- seq(0, 1, length.out = 10)
guessing <- map_df(probs, function(p){
  y_hat <- 
    sample(c("Male", "Female"), n, replace = TRUE, prob=c(p, 1-p)) %>% 
    factor(levels = c("Female", "Male"))
  list(method = "Guessing",
       FPR = 1 - specificity(y_hat, test_set$sex),
       TPR = sensitivity(y_hat, test_set$sex))
})
guessing %>% qplot(FPR, TPR, data =., xlab = "1 - Specificity", ylab = "Sensitivity")

cutoffs <- c(50, seq(60, 75), 80)
height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
   list(method = "Height cutoff",
        FPR = 1-specificity(y_hat, test_set$sex),
        TPR = sensitivity(y_hat, test_set$sex))
})

# plot both curves together
bind_rows(guessing, height_cutoff) %>%
  ggplot(aes(FPR, TPR, color = method)) +
  geom_line() +
  geom_point() +
  xlab("1 - Specificity") +
  ylab("Sensitivity")

library(ggrepel) # for geom_text_repel

map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
   list(method = "Height cutoff",
        cutoff = x, 
        FPR = 1-specificity(y_hat, test_set$sex),
        TPR = sensitivity(y_hat, test_set$sex))
}) %>%
  ggplot(aes(FPR, TPR, label = cutoff)) +
  geom_line() +
  geom_point() +
  geom_text_repel(nudge_x = 0.01, nudge_y = -0.01)

# plot precision against recall
guessing <- map_df(probs, function(p){
  y_hat <- sample(c("Male", "Female"), length(test_index), 
                  replace = TRUE, prob=c(p, 1-p)) %>% 
    factor(levels = c("Female", "Male"))
  list(method = "Guess",
    recall = sensitivity(y_hat, test_set$sex),
    precision = precision(y_hat, test_set$sex))
})

height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Female", "Male"))
  list(method = "Height cutoff",
       recall = sensitivity(y_hat, test_set$sex),
    precision = precision(y_hat, test_set$sex))
})

bind_rows(guessing, height_cutoff) %>%
  ggplot(aes(recall, precision, color = method)) +
  geom_line() +
  geom_point()
guessing <- map_df(probs, function(p){
  y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE, 
                  prob=c(p, 1-p)) %>% 
    factor(levels = c("Male", "Female"))
  list(method = "Guess",
    recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
    precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})

height_cutoff <- map_df(cutoffs, function(x){
  y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
    factor(levels = c("Male", "Female"))
  list(method = "Height cutoff",
       recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
    precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})
bind_rows(guessing, height_cutoff) %>%
  ggplot(aes(recall, precision, color = method)) +
  geom_line() +
  geom_point()
```

## CONTINUOUS OUTCOMES: Evaluating the model with the Mean Suared Error and the Loss Functions

The general approach to defining best in machine learning is to define a loss function which can be applied to both categorical and continuous data.

The most commonly used loss function is the squared loss function.

If y hat is our prediction and y is the observed outcome, the squared loss function is simply the difference squared.

Because we often have a test set with many observation, say n observations, we use the mean squared error.

In practice, we often report the root mean square error, the RMSE, which is the square root of the MSE because it is in the same units as the outcomes. But doing the math is often easier with the MSE and is therefore more commonly used in textbooks, since these usually describe theoretical properties of the algorithms.

Note that if the outcomes are binary, both RMSE and MSE are equivalent to 1 minus accuracy since y hat minus y squared is 0 if the prediction was correct and 1 otherwise.

In general our goal is to build an algorithm that minimizes the loss so it is as close to 0 as possible. Because our data is usually a random sample, we can think of the MSE as a random variable and the observed MSE can be thought of as an estimate of the expected MSE. 
Note that this is a theoretical concept because in practice we only have one data set to work with. But in theory, we can think of having a very large number of random samples, call it B, apply our algorithm to each, obtain an MSE for each random sample, and then think of the expected MSE as the average or this equation.

There is an approach called cross validation that is used to estimate the MSE by trying to mimic the theoretical quantity. 

Note that there are other functions other than the squared loss. For example, the mean absolute error uses absolute values instead of squaring the errors. Here we focus on minimizing square loss since it is the most widely used.
